{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCL BIOC0016 - Bioinformatics\n",
    "Alan R. Lowe (a.lowe@ucl.ac.uk)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Bioimage informatics / Machine learning\n",
    "\n",
    "The aim of the exercises in this notebook is to familiarise you with the different steps of evaluating the performance of a simple convolutional neural network (CNN). You will use a real CNN to classify different cell states using image data provided to you. A more sophisticated version of this neural network has been used in recently published research. The architecture of the network is similar to those you will have learnt about in the lectures.\n",
    "\n",
    "The CNN has been pre-trained on a dataset of real images from data collected at UCL. This is a very simple CNN, and you are going to assess the performance of it by making predictions of the cell state using image data, and comparing these with your ground truth annotation.  The images that you are provided with are called a 'hold out' set, since they have come from the much larger original data set, but have not been used to train the neural network. The CNN has not 'seen' these data during the training phase, and therefore represent a real test of the performance of the network.  \n",
    "\n",
    "The practical contains following sections:\n",
    "\n",
    "1. Data annotation\n",
    "2. Make predictions with a convolutional neural network\n",
    "3. Compare these with the data that you have annotated\n",
    "4. Determine the accuracy of the model\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Code sections are coloured according to the following scheme:\n",
    "\n",
    "* <div class=\"task_red\"> Code that needs to be written by you. </div>\n",
    "* <div class=\"task_blue\"> Code that needs to be edited by you, perhaps changing some parameters. </div>\n",
    "* <div class=\"task_green\"> A task that needs to be completed by you. This may be recording the results. </div>\n",
    "\n",
    "\n",
    "### Be part of the research project!\n",
    "\n",
    "This is based on a real research project, and you can contribute to the project by recording your results as part of the practical. You can read more about the research project [here](http://lowe.cs.ucl.ac.uk/cellx.html).\n",
    "\n",
    "As you complete different sections of the practical, we ask you to complete different sections of a google form. By recording the results, you can contribute to the research programme.\n",
    "\n",
    "### Further reading\n",
    "* http://lowe.cs.ucl.ac.uk/cellx.html\n",
    "* https://en.wikipedia.org/wiki/Bioimage_informatics\n",
    "* https://teachablemachine.withgoogle.com/train/image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICAL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 - Data Annotation\n",
    "\n",
    "\n",
    "To test how well the neural network is able to perform on unseen images, we need to manually annotate the images with a label. We can then count how many labels the neural network correctly predicts. In this section, you will be provided with a random sample of (n=25) unlabeled images. Using the guide below, please annotate each image with one of the six labels provided. If you are unsure, use the 'Unknown' label.\n",
    "\n",
    "You will store these annotations using two Python data structures, a `list` and a `dictionary`.\n",
    "\n",
    "A `list` is just what it sounds like, a sequence of items in order. A list can have multiple entries (separated by commas), or be empty:\n",
    "```python\n",
    "list_of_numbers = [0, 1, 2, 3, 4]\n",
    "empty_list = []\n",
    "```\n",
    "\n",
    "A `dictionary` stores key-value pairs.\n",
    "\n",
    "The final data structure may look something like this:\n",
    "\n",
    "```python \n",
    "annotation = {'interphase': [0,5,3,20], \n",
    "              'prometaphase': [7,2], \n",
    "              'metaphase': [14,33,6], \n",
    "              'anaphase': [508,1], \n",
    "              'apoptosis': [8], \n",
    "              'unknown': []}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up plotting and import some useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helpers' has no attribute 'css_styling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9e91593ce321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# style the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'helpers' has no attribute 'css_styling'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sample of images from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, image_idx = helpers.get_example_images(num_images=1)\n",
    "helpers.plot_images(images, image_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_blue\"> <b>TASK</b>: try changing the number of images. For the remainder of the practical, you will need at least 25 images.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_red\">\n",
    "    <b>TASK:</b> Complete the annotation dictionary. To do this you will need to make a list of the image numbers, and your best guess as to the label that each image should have. Once you have this, compile the information into a dictionary structure.           \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the annotation\n",
    "helpers.validate_annotation(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_green\">\n",
    "    <b>TASK:</b> Complete the <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdhPIHCMvtHVUCe-7_XYHujWIW2mq7SzcA2mfYh6LHiZ2LUPQ/viewform\">Google form</a>, to record the annotations you have made\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 - Using the CNN to make predictions\n",
    "---\n",
    "\n",
    "\n",
    "The (pseudo)code for the neural network looks like this:\n",
    "\n",
    "```python\n",
    "\n",
    "def simple_CNN(convolutional_kernels=32):\n",
    "    \"\"\" simple_CNN\n",
    "    \n",
    "    Build a simple convolutional classifier, that returns a one-hot classification (5,)\n",
    "    \n",
    "    Args:\n",
    "        convolutional_kernels: the number of kernels for the conv layers\n",
    "        \n",
    "    Returns:\n",
    "        a keras model of the CNN\n",
    "    \"\"\"\n",
    "    # first, set up the input layer\n",
    "    image = Input(shape=[32, 32, 1], name=\"input\")\n",
    "\n",
    "    # convolutions and pooling\n",
    "    conv1 = Conv2D(convolutional_kernels, (3, 3), activation='relu', padding='valid')(image)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(convolutional_kernels, (3, 3), activation='relu', padding='valid')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    # flatten the data and fully connect to output layer\n",
    "    flat1 = Flatten()(pool2)\n",
    "    logits = Dense(5, activation='linear')(flat1)\n",
    "    softmax = Activation('softmax', name='output')(logits)\n",
    "\n",
    "    # return the full model\n",
    "    return Model(inputs=image, outputs=softmax, name=\"Convolutional_Neural_Network\")\n",
    "```\n",
    "\n",
    "In the following lines of code `helpers.load_CNN_model()` builds the network shown above, and sets the weights and biases using the pretrained values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = helpers.load_CNN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictions = model.predict(np.stack([im[...,np.newaxis] for im in images], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(predictions, axis=1)\n",
    "print(y_pred, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, y in enumerate(y_pred):\n",
    "    print(f'Image #{image_idx[i]:<5} --> {helpers.STATES[y]:>12} ({predictions[i,y]:.5f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
