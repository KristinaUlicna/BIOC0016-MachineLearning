{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"./files/style.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"./files/style.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCL BIOC0016 - Bioinformatics\n",
    "Alan R. Lowe (a.lowe@ucl.ac.uk)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Bioimage informatics / Machine learning\n",
    "\n",
    "The aim of the exercises in this notebook is to familiarise you with the different steps of evaluating the performance of a simple convolutional neural network (CNN). You will use a real CNN to classify different cell states using image data provided to you. A more sophisticated version of this neural network has been used in recently published research. The architecture of the network is similar to those you will have learnt about in the lectures.\n",
    "\n",
    "The network is able to tell whether a cell is proliferating or dead.\n",
    "\n",
    "The CNN has been pre-trained on a dataset of real images from data collected at UCL. This is a very simple CNN, and you are going to assess the performance of it by making predictions of the cell state using image data, and comparing these with your ground truth annotation.  The images that you are provided with are called a 'hold out' set, since they have come from the much larger original data set, but have not been used to train the neural network. The CNN has not 'seen' these data during the training phase, and therefore represent a real test of the performance of the network.  \n",
    "\n",
    "The practical contains following sections:\n",
    "\n",
    "1. Data annotation\n",
    "2. Make predictions with a convolutional neural network\n",
    "3. Compare these with the data that you have annotated\n",
    "4. Determine the accuracy of the model\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Code sections are coloured according to the following scheme:\n",
    "\n",
    "* <div class=\"task_red\"> Code that needs to be written by you. </div>\n",
    "* <div class=\"task_blue\"> Code that needs to be edited by you, perhaps changing some parameters. </div>\n",
    "* <div class=\"task_green\"> A task that needs to be completed by you. This may be recording the results. </div>\n",
    "\n",
    "\n",
    "### Be part of the research project!\n",
    "\n",
    "This is based on a real research project, and you can contribute to the project by recording your results as part of the practical. You can read more about the research project [here](http://lowe.cs.ucl.ac.uk/cellx.html).\n",
    "\n",
    "As you complete different sections of the practical, we ask you to complete different sections of a google form. By recording the results, you can contribute to the research programme.\n",
    "\n",
    "### Further reading\n",
    "* http://lowe.cs.ucl.ac.uk/cellx.html\n",
    "* https://en.wikipedia.org/wiki/Bioimage_informatics\n",
    "* https://teachablemachine.withgoogle.com/train/image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PRACTICAL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 1 - Data Annotation\n",
    "\n",
    "\n",
    "To test how well the neural network is able to perform on unseen images, we need to manually annotate some new and unseen images with a label. We can then count how many labels the neural network correctly predicts. In this section, you will be provided with a random sample of unlabeled images. You will need at least 25, although the more the better. Using the guide below, please annotate each image with one of the six labels provided. If you are unsure, use the 'Unknown' label.\n",
    "\n",
    "Here are some examples of cells and their corresponding labels:\n",
    "\n",
    "![Cell state labels](./files/cell_states.png \"Title\")\n",
    "\n",
    "\n",
    "You will store these annotations using two Python data structures, a `list` and a `dictionary`.\n",
    "\n",
    "A `list` is just what it sounds like, a sequence of items in order. A list can have multiple entries (separated by commas), or be empty:\n",
    "```python\n",
    "list_of_numbers = [0, 1, 2, 3, 4]\n",
    "empty_list = []\n",
    "```\n",
    "\n",
    "A `dictionary` stores key-value pairs.\n",
    "\n",
    "The final data structure may look something like this:\n",
    "\n",
    "```python \n",
    "annotation = {'interphase': [0,5,3,20], \n",
    "              'prometaphase': [7,2], \n",
    "              'metaphase': [14,33,6], \n",
    "              'anaphase': [508,1], \n",
    "              'apoptosis': [8], \n",
    "              'unknown': []}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up plotting and import some useful libraries\n",
    "\n",
    "We first need to load some libraries of code that will help with the data processing and visualization. If you are more experienced at Python, you can read the code for the `helpers` library at [github](https://github.com/quantumjot/BIOC0016-MachineLearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import helpers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sample of images from the test dataset\n",
    "\n",
    "First, we need to get a random sample of images from the dataset. The dataset has thousands of images in it. The first command `images = helpers.get_example_images()` uses the `helpers` library to call a function `get_example_images`. This requests images from the dataset and places them in a list called `images`.\n",
    "\n",
    "The second command `helpers.plot_images()` takes the list of images as an argument and plots them in a matrix, showing the unique ID numbers above them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images = helpers.get_example_images(num_images=10)\n",
    "helpers.plot_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_blue\"> <b>TASK</b>: Try changing the number of images. For the remainder of the practical, you will need at least 25 images.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your own sample of images, you need to annotate them to create the **ground truth**. \n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div class=\"task_red\">\n",
    "    <b>TASK:</b> Complete the annotation dictionary. To do this you will need to make a list of the image numbers, and your best guess as to the label that each image should have. Once you have this, compile the information into a dictionary structure, and run the validation to make sure that the annotation format is correct. If the validation returns True, you can copy the annotation cell to the form.          \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the annotation\n",
    "helpers.validate_annotation(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_green\">\n",
    "    <b>TASK:</b> Complete the <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdhPIHCMvtHVUCe-7_XYHujWIW2mq7SzcA2mfYh6LHiZ2LUPQ/viewform\">Google form</a>, to record the annotations you have made\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 2 - Using the CNN to make predictions\n",
    "\n",
    "### Loading the pre-trained model\n",
    "\n",
    "In the following lines of code `helpers.load_CNN_model()` builds the CNN network, and sets the weights and biases using the pretrained values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = helpers.load_CNN_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='task_red'> <b>TASK:</b> Summarise the model and get the number of parameters. Answer the following questions:\n",
    "    <ul>\n",
    "        <li>How many convolutional layers are there? </li>\n",
    "        <li>How many kernels are used in each convolutional layer? </li>\n",
    "        <li>How many output classes are there? </li>\n",
    "        <li>What is the size of the input image, and the activations after the convolutional layers?</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**HINT**: You can use the command `model.summary()` to get the details of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise activations within the network\n",
    "\n",
    "When the network is given an example, we can visualize the activations within the networks from the sample. To visualize the activations within the network, you can use the `helpers.visualize_layers()` command.\n",
    "\n",
    "To do this, the function needs three (3) arguments:\n",
    "`helpers.visualize_layers(model, image, layer)`\n",
    "\n",
    "<br />\n",
    "\n",
    "<div class='task_red'><b>TASK:</b> visualize the activations within the network for the first and second convolutional layers (layer=0 or layer=1). </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the predictions\n",
    "\n",
    "\n",
    "Now that we have loaded the pre-trained model. We can feed it the same images that you have labeled and generate predictions for the label. The predictions are returned as the probability of the label given the image data ($\\text{P}(\\text{label} | \\text{data})$). In fact, the model returns the probability for *all* labels given the data.\n",
    "\n",
    "<br />\n",
    "<div class=\"task_blue\"> <b>TASK:</b> Using the model, make predictions for the images that you have annotated.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(np.concatenate([im.as_tensor() for im in images], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the images and their predictions using one of the helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.plot_predictions(images, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 3 - Compare the predictions with your annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# helpers.plot_predictions(images, predictions, annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(predictions, axis=1)\n",
    "print(y_pred, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, y in enumerate(y_pred):\n",
    "    print(f'Image #{images[i].ID:<5} --> {helpers.STATES[y]:>12} ({predictions[i,y]:.5f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 4 - Calculate the accuracy of the model using your ground truth data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"task_red\"><b>TASK:</b> Please make sure you have completed the Google form and uploaded your results </div>\n",
    "\n",
    "# End of the practical\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
